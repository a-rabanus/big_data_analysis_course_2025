{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "524798dc",
   "metadata": {},
   "source": [
    "# Image classifier approach\n",
    "\n",
    "This notebook describes my approach to image classification by some characteristics\n",
    "\n",
    "## Features\n",
    "\n",
    "### Color simmilarities\n",
    "\n",
    "- Convert rgb values to light frequency\n",
    "- plot values as curve to get color spectrum of the image\n",
    "    - bin size for counting controls precision\n",
    "- pick three highest peaks\n",
    "- find image with the most similar peak values\n",
    "\n",
    "### Feature similarities\n",
    "\n",
    "- using ```cv.approxPolyDP()``` we find primitives in the image\n",
    "    - [stackoverflow problem description](https://stackoverflow.com/questions/11424002/how-to-detect-simple-geometric-shapes-using-opencv)\n",
    "- make list of primitives with type, average color, position and size\n",
    "- compare with other images, turning on and off different similarity filters\n",
    "\n",
    "## Similarity rating\n",
    "\n",
    "Selecting the different aspects should give some normalized rating of how similar the images are."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be1a82c",
   "metadata": {},
   "source": [
    "## Addition\n",
    "### Overall\n",
    "I agree that adding different options that the user can turn on or off would make the system more flexible.\n",
    "\n",
    "If we have time at the end, we could try building a GUI so the user can interact with these options.\n",
    "\n",
    "It could also be useful to let the user adjust settings, like how many top colors to compare (e.g., top 1, 2, 3â€¦), or choose between histogram-based or cluster-based approaches for color comparison.\n",
    "\n",
    "\n",
    "### Some additions and thoughts:\n",
    "- not sure if \"light frequency\" == HSV? \n",
    "- I would definetly use HSV\n",
    "- we could cluster all HSV values using KMeans to find the main colors in the image. The resulting clusters would then contain both: color centers and their relative weight so that we can compare the overall color distribution in the images.\n",
    "\n",
    "\n",
    "\n",
    "### To find similarities in where those colors are located in the image:\n",
    "- we can add pixel coordinates as a feature in the KMeans clustering.\n",
    "OR \n",
    "- a blurred version of the reference image and use it as a soft mask to compare how colors are spread out in the image. --> general idea of the color layout without focusing on exact color matches.\n",
    "OR\n",
    "- (I think you mentioned something like this before) split the image into a grid and calculate the main color for each section. -> low-resolution color map to compare with maps from other images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0082becf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports and path to images\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import euclidean_distances, cosine_similarity\n",
    "\n",
    "dir = r\"C:\\Users\\anton\\OneDrive\\Documents\\HSD\\sem4\\DAISY_2025_images_for_bigdata\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555f23cd",
   "metadata": {},
   "source": [
    "### Loading Images with OpenCV and displaying them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8c9b6c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_from_directory(directory):\n",
    "    \"\"\"\n",
    "    Load all images from the specified directory using OpenCV.\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    for filename in os.listdir(directory):\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        if os.path.isfile(filepath):\n",
    "            img = cv2.imread(filepath)\n",
    "            if img is not None:\n",
    "                images.append((filename, img))\n",
    "    return images\n",
    "\n",
    "def display_image(images, img_id=None):\n",
    "    \"\"\"\n",
    "    Display a list of images using Matplotlib.\n",
    "    \"\"\"\n",
    "    if img_id is None:\n",
    "        img_id = random.randint(0, len(images) - 1)\n",
    "    else:\n",
    "        img_id = img_id\n",
    "    filename, img = images[img_id]\n",
    "    plt.figure()\n",
    "    plt.title(filename)\n",
    "    # Convert BGR to RGB for displaying with Matplotlib\n",
    "    plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    pass\n",
    "\n",
    "def downscale_image(image, factor=0.01):\n",
    "    \"\"\"\n",
    "    Downscale images by a specified factor.\n",
    "    \"\"\"\n",
    "    filename, img = image\n",
    "    height, width = img.shape[:2]\n",
    "    new_size = (int(width * factor), int(height * factor))\n",
    "    downscaled_img = cv2.resize(img, new_size, interpolation=cv2.INTER_AREA)\n",
    "    return (filename, downscaled_img)\n",
    "\n",
    "def extract_dominant_colors(image, n_clusters=3):\n",
    "    \"\"\"\n",
    "    Extract dominant colors from an image using KMeans clustering.\n",
    "    Returns a n_clusters long list of rgb values \n",
    "    \"\"\"\n",
    "    # Unpack the tuple to get the filename and image\n",
    "    # Convert the image to RGB (from BGR)\n",
    "    # Reshape the image to a 2D array of pixels where each row is a pixel and each column is a color channel\n",
    "\n",
    "    filename, img = image\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "    pixels = img.reshape(-1, 3)\n",
    "    \n",
    "    # Apply KMeans clustering\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    kmeans.fit(pixels)\n",
    "    \n",
    "    # Get the cluster centers (dominant colors)\n",
    "    dominant_colors = kmeans.cluster_centers_.astype(int)\n",
    "    labels = kmeans.labels_\n",
    "    cluster_sizes = np.bincount(labels)\n",
    "\n",
    "    # Get indices that would sort the cluster sizes in descending order\n",
    "    sorted_indices = np.argsort(cluster_sizes)[::-1]\n",
    "\n",
    "    # Sort both colors and cluster sizes using these indices\n",
    "    dominant_colors = dominant_colors[sorted_indices]\n",
    "    cluster_sizes = cluster_sizes[sorted_indices]\n",
    "    \n",
    "    # to percentages\n",
    "    total_pixels = sum(cluster_sizes)\n",
    "    percentages = (cluster_sizes / total_pixels) * 100\n",
    "\n",
    "    return (filename, dominant_colors, percentages)\n",
    "\n",
    "def plot_dominant_colors(all_colors, img_id=None):\n",
    "    \"\"\"\n",
    "    Gets tuple with label and dominant colors and plots them\n",
    "    \"\"\"\n",
    "    if img_id is None:\n",
    "        img_id = random.randint(0, len(all_colors) - 1)\n",
    "    else:\n",
    "        img_id = img_id\n",
    "        \n",
    "    filename, colors, percentages = all_colors[img_id]\n",
    "    \n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(8, 2))\n",
    "    plt.axis('off')\n",
    "    plt.imshow([colors / 255])  # Normalize colors to [0, 1] for Matplotlib\n",
    "    plt.title(filename)\n",
    "    \n",
    "    # Annotate with percentages\n",
    "    for i, (color, percentage) in enumerate(zip(colors, percentages)):\n",
    "        plt.text(\n",
    "            x=i / len(colors) + 0.17,  # Position text slightly offset\n",
    "            y=0.5,\n",
    "            s=f\"{percentage:.1f}%\",\n",
    "            color=\"white\" if np.mean(color) < 128 else \"black\",  # Contrast text color\n",
    "            fontsize=12,\n",
    "            ha=\"center\",\n",
    "            va=\"center\",\n",
    "            transform=plt.gca().transAxes\n",
    "        )\n",
    "    \n",
    "    plt.show()\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde215a6",
   "metadata": {},
   "source": [
    "### Clustering using KMeans\n",
    "\n",
    "While clustering, I think it makes sense to choose attributes that have high value differences between images.\n",
    "\n",
    "Because most images were taken using a smartphone, it makes little sense to choose `saturation` or `value` from `HSV`, because the automatic exposure and image post processing of smartphones ensures that these are similar across all images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f1046615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and downscale images\n",
    "images = load_images_from_directory(dir)\n",
    "\n",
    "# downscale images\n",
    "downscaled_images = []\n",
    "for img in images:\n",
    "    downscaled_images.append(downscale_image(img))\n",
    "# print(f\"Downscaled {len(images)} images\")\n",
    "\n",
    "# extract dominant colors for all images\n",
    "all_dominant_colors = []\n",
    "for img in downscaled_images:\n",
    "    all_dominant_colors.append(extract_dominant_colors(img, n_clusters=3))\n",
    "# print(all_dominant_colors[testing_id][0])\n",
    "\n",
    "# normalized\n",
    "all_dominant_colors_normalized = []\n",
    "for dominant_colors in all_dominant_colors:\n",
    "    normalized_colors = dominant_colors[1] / 255\n",
    "    norm_label = dominant_colors[0]\n",
    "    percentages = dominant_colors[2]\n",
    "    all_dominant_colors_normalized.append((norm_label, normalized_colors, percentages))\n",
    "# print(all_dominant_colors_normalized[testing_id][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c348e386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show progress of the workflow\n",
    "testing_id = random.randint(0, len(images) - 1)\n",
    "\n",
    "# display_image(images, img_id=testing_id)\n",
    "# display_image(downscaled_images, img_id=testing_id)\n",
    "# plot_dominant_colors(all_dominant_colors, img_id=testing_id)\n",
    "# print(all_dominant_colors_normalized[testing_id][0])\n",
    "\n",
    "\n",
    "# all_dominant_colors_normalized is a list of tuples\n",
    "# tuple contains: n_cluster x rgb numpy array, img label.\n",
    "\n",
    "# for i in range(len(all_dominant_colors)):\n",
    "#     plot_dominant_colors(all_dominant_colors, img_id=i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f591d3f",
   "metadata": {},
   "source": [
    "### Categorizing according to color similarity\n",
    "\n",
    "To compare the `3 x n` arrays (where each row represents a color in normalized RGB format), we'll look into using some of the following methods:\n",
    "\n",
    "#### Euclidean Distance:\n",
    "- Treat each color as a point in 3D space (R, G, B).\n",
    "- Calculate the Euclidean distance between the colors. Smaller distances indicate higher similarity.\n",
    "\n",
    "$$d = \\sqrt{(R_1 - R_2)^2 + (G_1 - G_2)^2 + (B_1 - B_2)^2}$$\n",
    "> source: [Wolfram MathWorld - distance](https://mathworld.wolfram.com/Distance.html)\n",
    "\n",
    "Because we have multiple colors for each image we should compare the most dominant dominant of each respective image.\n",
    "\n",
    "\n",
    "#### Cosine Similarity:\n",
    "- Treat each color as a vector in 3D space.\n",
    "- Compute the cosine of the angle between two vectors. A value closer to 1 indicates higher similarity.\n",
    "\n",
    "#### Thresholding:\n",
    "- Define a threshold for similarity (e.g., a maximum allowable distance).\n",
    "- If the distance between two colors is below the threshold, consider them similar.\n",
    "\n",
    "#### Other options:\n",
    "##### Color Difference Metrics:\n",
    "- Use perceptual color difference formulas like CIEDE2000 or CIE76, which account for how humans perceive color differences.\n",
    "\n",
    "##### Cluster Matching:\n",
    "- If comparing two sets of colors (e.g., two images), match the closest colors between the sets using a distance metric (e.g., Euclidean).\n",
    "- Sum the distances for all matches to get an overall similarity score.\n",
    "\n",
    "##### Weighted Comparison:\n",
    "- Assign weights to the RGB channels based on their perceptual importance (e.g., green is more sensitive to the human eye).\n",
    "- Use a weighted distance metric for comparison.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "By combining these methods, you can create a robust similarity measure for your use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4def4980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.62303594 0.52814664 0.31259601]\n",
      "[4.32157968e-05 8.07422093e-02 8.91767287e-03]\n"
     ]
    }
   ],
   "source": [
    "def euk_dis(img_1, img_2):\n",
    "    \"\"\"\n",
    "    Calculate the Euclidean distance between the colors. Smaller distances indicate higher similarity.\n",
    "\n",
    "    dist(x, y) = sqrt(dot(x, x) - 2 * dot(x, y) + dot(y, y))\n",
    "\n",
    "    [Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.euclidean_distances.html)\n",
    "    \n",
    "    this function outputs the distance from every entry to every other entry.\n",
    "    We're only interested in the diagonal of this matrix\n",
    "\n",
    "\n",
    "    input: img_1 (numpy array), img_2 (numpy array)\n",
    "    \n",
    "    output: float for every distande\n",
    "    \n",
    "    output interpretation: `0`: Identical colors, âˆš3: Maximum possible distance for 3 dim\n",
    "    \"\"\"\n",
    "    all_distances_between_images = euclidean_distances(img_1, img_2)\n",
    "    distances = np.diag(all_distances_between_images)\n",
    "    return distances.T\n",
    "\n",
    "def cos_dis(img_1, img_2):\n",
    "    \"\"\"\n",
    "    cosine_distance = 1 - cosine_similarity\n",
    "\n",
    "    K(X, Y) = <X, Y> / (||X||*||Y||)\n",
    "\n",
    "    [Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_similarity.html)\n",
    "    \n",
    "    this function outputs the cosine similarity from every entry to every other entry.\n",
    "    We're only interested in the diagonal of this matrix\n",
    "\n",
    "    input: img_1 (numpy array), img_2 (numpy array)\n",
    "    \n",
    "    output: float\n",
    "    \n",
    "    output interpretation: 0 = identical, 2 = maximally dissimilar.\n",
    "    \"\"\"\n",
    "    all_similarities_between_images = cosine_similarity(img_1, img_2)\n",
    "    similarity = np.diag(all_similarities_between_images)\n",
    "    return [1, 1, 1] - similarity\n",
    "\n",
    "\n",
    "print(euk_dis(all_dominant_colors_normalized[0][1],\n",
    "        all_dominant_colors_normalized[1][1]))\n",
    "print(cos_dis(all_dominant_colors_normalized[0][1],\n",
    "        all_dominant_colors_normalized[1][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662f9a4e",
   "metadata": {},
   "source": [
    "# Main comparison code-block\n",
    "This code snippet compares the `euk_dis` and `cos_sim` and plots the `n` most similar images in the dataset.\n",
    "\n",
    "hands over `k` most similar images to input image, based on euk_dis and cos_sim\n",
    "returns labels of images that fit\n",
    "\n",
    "## Similarity calculation approach\n",
    "\n",
    "calculate the `euk_dis` or `cos_sim` for each cluster in the dominant_colors, making sure to compare the largest clusters, and moving on to the next smallest cluster.\n",
    "\n",
    "To make sure that the the weight of each cluster distance is **not** equal we use a function to ensure proper similarity weight distribution.\n",
    "\n",
    "The first implemenation will use a simple factor infront of the cluster similarity score:\n",
    "\n",
    "`sim_score_0 * (1 - (0/n_clusters)` + `sim_score_1 * (1 - (1/n_clusters)` + `...` + `sim_score_n * (1 - (n/n_clusters)`\n",
    "\n",
    "later on the `(1 - (0/n_clusters)` could be swapped for a more fitting weights function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0af341a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# similarity(input_image, all_images):\n",
    "\"\"\"\n",
    "hands over 5 most similar images to input image, based on euk_dis and cos_sim\n",
    "\n",
    "returns labels of images that fit\n",
    "\"\"\"\n",
    "testing_id\n",
    "\n",
    "def image_comparison_cos_dis():\n",
    "\n",
    "    pass\n",
    "\n",
    "def image_comparison_euk_dis():\n",
    "\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83555cc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9529e827",
   "metadata": {},
   "source": [
    "# Future improvements:\n",
    "\n",
    "n_clusters could be used to generate some similarity threshold\n",
    "\n",
    "$\\blacktriangleright$ **hypothesis:** image similarity might increase with `n_cluster`.\n",
    "\n",
    "$\\blacktriangleright$ $\\blacktriangleright$ plot `n_clusters` against `similarity_score` to see when an increase in cluster size is no longer significant"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
